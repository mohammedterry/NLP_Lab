{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammedterry/NLP_Lab/blob/master/NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "BINfboLPqcO_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "example_document = '''\n",
        "Friday 9/11/2018\n",
        "9th November 2018\n",
        "\n",
        "Hey ho !\n",
        "Can i have your contact details, please? \n",
        "aaha aaahhhh ah ahah ahahah ahh ahhahahaha hahaha hahahah\n",
        "Sure. \n",
        "My email is mohammed@cognitionx.io and my phone number is 0743423453 0 01 012 1987 98 0.32 2.34 \n",
        "\n",
        "The Matrix is a 1999 science fiction action film written and directed by The Wachowskis, starring Keanu Reeves, Laurence Fishburne, Carrie-Anne Moss, Hugo Weaving, and Joe Pantoliano.\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wy5wwTpCuutS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Flair"
      ]
    },
    {
      "metadata": {
        "id": "1zkh_P6buwRq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install flair"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_eEPh4_f9cXN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from flair.models import SequenceTagger\n",
        "flair_12class = SequenceTagger.load('ner-ontonotes-fast')\n",
        "flair_4class = SequenceTagger.load('ner')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bqHVPQSIu7j_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from flair.data import Sentence\n",
        "\n",
        "s = Sentence(example_document)\n",
        "flair_12class.predict(s)\n",
        "s.to_dict(tag_type='ner')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vbzepNzhzVkY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def flair_4ner(document):\n",
        "  s = Sentence(document)\n",
        "  flair_4class.predict(s)\n",
        "  entities = s.to_dict(tag_type='ner')\n",
        "  return [(entity[\"text\"], entity[\"type\"]) for entity in entities[\"entities\"]]\n",
        "\n",
        "def flair_12ner(document):\n",
        "  s = Sentence(document)\n",
        "  flair_12class.predict(s)\n",
        "  entities = s.to_dict(tag_type='ner')\n",
        "  return [(entity[\"text\"], entity[\"type\"]) for entity in entities[\"entities\"]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3kcqsnNbz6h9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "flair_4ner(example_document)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VJsXDyy005ys",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "flair_12ner(example_document)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wOO_vNe32Ze6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Deep Pavlov"
      ]
    },
    {
      "metadata": {
        "id": "7xLK6na62sY1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install deeppavlov\n",
        "!python3 -m deeppavlov install ner_ontonotes\n",
        "from deeppavlov import configs, build_model\n",
        "deeppavlov_ner = build_model(configs.ner.ner_ontonotes, download=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h2t_l-Lg3hYF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convert_entities(entities):\n",
        "  ents = set()\n",
        "  for entity,next_entity in zip(entities,entities[1:] + [(\".\",\"O\")]):\n",
        "    word,tag = entity\n",
        "    if tag != \"O\":\n",
        "      ent_position, ent_type = tag.split(\"-\")\n",
        "      if ent_position == \"U\":\n",
        "        ents.add((word,ent_type))\n",
        "      else:\n",
        "        if ent_position == \"B\":\n",
        "          w = word\n",
        "        elif ent_position == \"I\":\n",
        "          w += \" \" + word\n",
        "          if next_entity[1].split(\"-\")[0] != \"I\":\n",
        "            ents.add((w,ent_type))\n",
        "  return ents\n",
        "\n",
        "def dp_ner(sentence):\n",
        "  tokens,tags = deeppavlov_ner([sentence])\n",
        "  return convert_entities([(tok,tg) for token,tag in zip(tokens,tags) for tok,tg in list(zip(token,tag)) ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_RozDHFQ6yHv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dp_ner(example_document)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "btx1denTqF1_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# GATE API (Free)"
      ]
    },
    {
      "metadata": {
        "id": "wvmXB7UKqIeg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import requests\n",
        "url = \"https://cloud-api.gate.ac.uk/process-document/annie-named-entity-recognizer\"\n",
        "headers = {'Content-Type': 'text/plain'}\n",
        "response = requests.post(url, data=example_document, headers=headers).json()\n",
        "\n",
        "import json\n",
        "print(json.dumps(response, indent=2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sj_XZJgCqfbW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gate_ner(sentence):\n",
        "  import requests\n",
        "  return [(sentence[entity[\"indices\"][0]:entity[\"indices\"][1]] + f\" ({entity['gender']})\",entity_type) if entity_type == \"Person\" and \"gender\" in entity else (sentence[entity[\"indices\"][0]:entity[\"indices\"][1]],entity_type)  for entity_type,entities in requests.post(\"https://cloud-api.gate.ac.uk/process-document/annie-named-entity-recognizer\", data=sentence, headers={'Content-Type': 'text/plain'}).json()[\"entities\"].items() for entity in entities]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M10blkrErQ28",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gate_ner(example_document)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HBqxcYavqoah",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Stanford Core NLP"
      ]
    },
    {
      "metadata": {
        "id": "U6in8psMKLMs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install nltk==3.2.4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W8w7LLsyKAYU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/software/stanford-ner-2015-04-20.zip\n",
        "!unzip stanford-ner-2015-04-20.zip "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NSfCCO9PKQ27",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.tag.stanford import StanfordNERTagger\n",
        "jar = \"stanford-ner-2015-04-20/stanford-ner-3.5.2.jar\"\n",
        "model = \"stanford-ner-2015-04-20/classifiers/\" \n",
        "st_3class = StanfordNERTagger(model + \"english.all.3class.distsim.crf.ser.gz\", jar, encoding='utf8') \n",
        "st_4class = StanfordNERTagger(model + \"english.conll.4class.distsim.crf.ser.gz\", jar, encoding='utf8') \n",
        "st_7class = StanfordNERTagger(model + \"english.muc.7class.distsim.crf.ser.gz\", jar, encoding='utf8') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RYow2ZnRKURv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def stanford_ner(document,model):\n",
        "  if model == 1:\n",
        "    return [(entity,tag) for entity,tag in st_3class.tag(document.split()) if tag != \"O\"]\n",
        "  elif model == 2:\n",
        "    return [(entity,tag) for entity,tag in st_4class.tag(document.split()) if tag != \"O\"]\n",
        "  elif model == 3:\n",
        "    return [(entity,tag) for entity,tag in st_7class.tag(document.split()) if tag != \"O\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F3tqJYjGKaaP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stanford_ner(example_document,model=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VkPAFGdEOMrl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stanford_ner(example_document,model=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "12cZITULOQ6u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stanford_ner(example_document,model=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HdHi9MLEt6OB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# NLTK"
      ]
    },
    {
      "metadata": {
        "id": "dLtIbPYJuAfe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7r0uSb7LqwKF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def nltk_ner(document):\n",
        "  return {(' '.join(c[0] for c in chunk), chunk.label() ) for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(document))) if hasattr(chunk, 'label') }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zPfbUR4QueDe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nltk_ner(example_document)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wjg7Sj3Tt8bz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Spacy"
      ]
    },
    {
      "metadata": {
        "id": "qkg1NC08xVYM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 -m spacy download en_core_web_lg\n",
        "import spacy\n",
        "sp = spacy.load('en_core_web_lg') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ghnQyFifqqeX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def spacy_ner(document):\n",
        "  return {(ent.text.strip(), ent.label_) for ent in sp(document).ents}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HSSFSJvVyxOX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "spacy_ner(example_document)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1k7BvEX_t-RL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Allen NLP"
      ]
    },
    {
      "metadata": {
        "id": "ojRC5XLQu8Ra",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install allennlp\n",
        "from allennlp.predictors import Predictor\n",
        "al = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/fine-grained-ner-model-elmo-2018.12.21.tar.gz\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Po5hHwi5puQj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convert_results(allen_results):\n",
        "  ents = set()\n",
        "  for word, tag in zip(allen_results[\"words\"], allen_results[\"tags\"]):\n",
        "    if tag != \"O\":\n",
        "      ent_position, ent_type = tag.split(\"-\")\n",
        "      if ent_position == \"U\":\n",
        "        ents.add((word,ent_type))\n",
        "      else:\n",
        "        if ent_position == \"B\":\n",
        "          w = word\n",
        "        elif ent_position == \"I\":\n",
        "          w += \" \" + word\n",
        "        elif ent_position == \"L\":\n",
        "          w += \" \" + word\n",
        "          ents.add((w,ent_type))\n",
        "  return ents\n",
        "\n",
        "def allennlp_ner(document):\n",
        "  return convert_results(al.predict(sentence=document))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hBSuqZ9Oq-XT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "allennlp_ner(example_document)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b2uvc2hb1zrI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Polyglot"
      ]
    },
    {
      "metadata": {
        "id": "6ixzM5Kn12Ae",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install -U git+https://github.com/aboSamoor/polyglot.git@master\n",
        "!polyglot download embeddings2.en ner2.en\n",
        "from polyglot.text import Text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DzUrNQKk2X7O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def polyglot_ner(document):\n",
        "  return {(' '.join(entity),entity.tag.split('-')[-1]) for entity in Text(document).entities}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BC9e3KFB3T93",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "polyglot_ner(example_document)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qwa_0BqYMVwx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Hierarchical Multi-Task Learning model (HMTL)"
      ]
    },
    {
      "metadata": {
        "id": "5i0ZDKU1IhOu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/huggingface/hmtl\n",
        "#!hmtl/scripts/data_setup.sh\n",
        "#!pip3 install emoji\n",
        "#!pip3 install allennlp\n",
        "#from hmtlPredictor import HMTLPredictor\n",
        "#html = HMTLPredictor(model_name =\"conll_full_elmo\")\n",
        "#hmtl.predict(example_document, raw_format = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nMAtuOgf5uAu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Which is the Best?"
      ]
    },
    {
      "metadata": {
        "id": "TUqUvqw88R5B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test = {\n",
        "  \"text\":'''Elizabeth was born in London as the first child of the Duke and Duchess of York, later King George VI and Queen Elizabeth, and she was educated privately at home. Her father acceded to the throne on the abdication of his brother King Edward VIII in 1936''',\n",
        "  \"entities\" : {\n",
        "    \"dates\":{\"1936\"},\n",
        "    \"locations\":{\"London\", \"York\"},\n",
        "    \"people\":{\n",
        "        \"Elizabeth\",\n",
        "        \"Duke\",\n",
        "        \"Duchess\",\n",
        "        \"King George VI\",\n",
        "        \"Queen Elizabeth\",\n",
        "        \"King Edward VIII\",\n",
        "    },\n",
        "  }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0RC_jDpEN9g2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Which is the quickest?"
      ]
    },
    {
      "metadata": {
        "id": "eBT6WbYl51gs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "dp_start = time.time()\n",
        "Y_deeppavlov = dp_ner(test[\"text\"])\n",
        "dp_end = time.time()\n",
        "deeppavlov_time = dp_end - dp_start\n",
        "print(f\"Deep Pavlov = {deeppavlov_time}s\")\n",
        "\n",
        "flair_start = time.time()\n",
        "Y_flair4 = flair_4ner(test[\"text\"])\n",
        "flair_end = time.time()\n",
        "flair_time4 = flair_end - flair_start\n",
        "print(f\"Flair (4 class) = {flair_time4}s\")\n",
        "\n",
        "flair_start = time.time()\n",
        "Y_flair12 = flair_12ner(test[\"text\"])\n",
        "flair_end = time.time()\n",
        "flair_time12 = flair_end - flair_start\n",
        "print(f\"Flair (12 class) = {flair_time12}s\")\n",
        "\n",
        "gate_start = time.time()\n",
        "Y_gate = gate_ner(test[\"text\"])\n",
        "gate_end = time.time()\n",
        "gate_time = gate_end - gate_start\n",
        "print(f\"GATE = {gate_time}s\")\n",
        "\n",
        "nltk_start = time.time()\n",
        "Y_nltk = nltk_ner(test[\"text\"])\n",
        "nltk_end = time.time()\n",
        "nltk_time = nltk_end - nltk_start\n",
        "print(f\"NLTK = {nltk_time}s\")\n",
        "\n",
        "stanford_start = time.time()\n",
        "Y_stanford1 = stanford_ner(test[\"text\"],model=1)\n",
        "stanford_end = time.time()\n",
        "stanford_time1 = stanford_end - stanford_start\n",
        "print(f\"Stanford Core NLP (3 class) = {stanford_time1}s\")\n",
        "\n",
        "stanford_start = time.time()\n",
        "Y_stanford2 = stanford_ner(test[\"text\"],model=2)\n",
        "stanford_end = time.time()\n",
        "stanford_time2 = stanford_end - stanford_start\n",
        "print(f\"Stanford Core NLP (4 class) = {stanford_time2}s\")\n",
        "\n",
        "stanford_start = time.time()\n",
        "Y_stanford3 = stanford_ner(test[\"text\"],model=3)\n",
        "stanford_end = time.time()\n",
        "stanford_time3 = stanford_end - stanford_start\n",
        "print(f\"Stanford Core NLP (7 class) = {stanford_time3}s\")\n",
        "\n",
        "spacy_start = time.time()\n",
        "Y_spacy = spacy_ner(test[\"text\"])\n",
        "spacy_end = time.time()\n",
        "spacy_time = spacy_end - spacy_start\n",
        "print(f\"Spacy = {spacy_time}s\")\n",
        "\n",
        "allen_start = time.time()\n",
        "Y_allen = allennlp_ner(test[\"text\"])\n",
        "allen_end = time.time()\n",
        "allen_time = allen_end - allen_start\n",
        "print(f\"Allen NLP = {allen_time}s\")\n",
        "\n",
        "poly_start = time.time()\n",
        "Y_polyglot = polyglot_ner(test[\"text\"])\n",
        "poly_end = time.time()\n",
        "poly_time = poly_end - poly_start\n",
        "print(f\"Polyglot = {poly_time}s\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_M4Zjho0-HM0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline  \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.rcdefaults()\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "n_words = len(test[\"text\"].split())\n",
        "ners = ('Polyglot', 'NLTK',  'Deep Pavlov','Spacy', 'Flair (12 class)', 'Flair (4 class)', 'GATE',  'Allen NLP', 'Stanford Core NLP (7 class)', 'Stanford Core NLP (4 class)', 'Stanford Core NLP (3 class)')\n",
        "performance = [ poly_time/n_words, nltk_time/n_words, deeppavlov_time/n_words, spacy_time/n_words, flair_time12/n_words, flair_time4/n_words, gate_time/n_words,  allen_time/n_words, stanford_time3/n_words, stanford_time2/n_words, stanford_time1/n_words]\n",
        "y_pos = np.arange(len(ners))\n",
        "\n",
        "ax.barh(y_pos, performance, align='center', color='green', ecolor='black')\n",
        "ax.set_yticks(y_pos)\n",
        "ax.set_yticklabels(ners)\n",
        "ax.invert_yaxis()  \n",
        "ax.set_xlabel('Time (s / word)')\n",
        "ax.set_title('How long does each NER take?')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GNn6V860OBY1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Which is the best NER for finding entities?"
      ]
    },
    {
      "metadata": {
        "id": "2dPZCVOO6dl5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def f1_score(Y, Y_hat, strict=False):\n",
        "  e = .000001\n",
        "  non_entities = {}\n",
        "  true_positives = sum([max([1 if y == y_hat else .5 if y_hat in y else .5 if y in y_hat else 0 for y_hat in Y_hat]+ [0]) for y in Y]) # correctly identified\n",
        "  if strict: true_positives = len(Y.intersection(Y_hat)) #if strict, only count exact entity matches and not partial matches (default)\n",
        "  dY = Y ^ Y_hat \n",
        "  false_positives = len(Y_hat.intersection(dY)) # incorrectly identified\n",
        "  false_negatives = len(Y) - true_positives # incorrectly rejected\n",
        "  recall = true_positives / (true_positives + false_negatives + e)\n",
        "  precision = true_positives / (true_positives + false_positives + e)\n",
        "  return (2 * precision * recall) / (precision + recall + e)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d0ipbqoRNaI5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "expected_entities = {entity for entities in test[\"entities\"].values() for entity in entities}\n",
        "gate_entities = {entity.replace('(male)','').replace('(female)','').strip() for entity,_ in Y_gate}\n",
        "polyglot_entities = {entity for entity,_ in Y_polyglot}\n",
        "spacy_entities = {entity for entity,_ in Y_spacy}\n",
        "nltk_entities = {entity for entity,_ in Y_nltk}\n",
        "allen_entities = {entity for entity,_ in Y_allen}\n",
        "stanford_entities1 = {entity for entity,_ in Y_stanford1}\n",
        "stanford_entities2 = {entity for entity,_ in Y_stanford2}\n",
        "stanford_entities3 = {entity for entity,_ in Y_stanford3}\n",
        "flair_entities4 = {entity for entity,_ in Y_flair4}\n",
        "flair_entities12 = {entity for entity,_ in Y_flair12}\n",
        "deeppavlov_entities = {entity for entity,_ in Y_deeppavlov}\n",
        "\n",
        "print(f\"Expected = 100.0%\\n\\t{sorted(expected_entities)}\\n\")\n",
        "f1_deeppavlov = f1_score(expected_entities, deeppavlov_entities)\n",
        "print(f\"Deep Pavlov = {f1_deeppavlov*100}%\\n\\t{sorted(deeppavlov_entities)}\\n\")\n",
        "f1_flair4 = f1_score(expected_entities, flair_entities4)\n",
        "print(f\"Flair (4 class) = {f1_flair4*100}%\\n\\t{sorted(flair_entities4)}\\n\")\n",
        "f1_flair12 = f1_score(expected_entities, flair_entities12)\n",
        "print(f\"Flair (12 class) = {f1_flair12*100}%\\n\\t{sorted(flair_entities12)}\\n\")\n",
        "f1_gate = f1_score(expected_entities, gate_entities)\n",
        "print(f\"GATE = {f1_gate*100}%\\n\\t{sorted(gate_entities)}\\n\")\n",
        "f1_poly = f1_score(expected_entities, polyglot_entities)\n",
        "print(f\"Polyglot = {f1_poly*100}%\\n\\t{sorted(polyglot_entities)}\\n\")\n",
        "f1_spacy = f1_score(expected_entities, spacy_entities)\n",
        "print(f\"Spacy = {f1_spacy*100}%\\n\\t{sorted(spacy_entities)}\\n\")\n",
        "f1_nltk = f1_score(expected_entities, nltk_entities)\n",
        "print(f\"NLTK = {f1_nltk*100}%\\n\\t{sorted(nltk_entities)}\\n\")\n",
        "f1_stanford1 = f1_score(expected_entities, stanford_entities1)\n",
        "print(f\"Stanford Core NLP (3 class) = {f1_stanford1*100}%\\n\\t{sorted(stanford_entities1)}\\n\")\n",
        "f1_stanford2 = f1_score(expected_entities, stanford_entities2)\n",
        "print(f\"Stanford Core NLP (4 class) = {f1_stanford2*100}%\\n\\t{sorted(stanford_entities2)}\\n\")\n",
        "f1_stanford3 = f1_score(expected_entities, stanford_entities3)\n",
        "print(f\"Stanford Core NLP (7 class) = {f1_stanford3*100}%\\n\\t{sorted(stanford_entities3)}\\n\")\n",
        "f1_allen = f1_score(expected_entities, allen_entities)\n",
        "print(f\"Allen NLP = {f1_allen*100}%\\n\\t{sorted(allen_entities)}\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YucNDasQX5Q2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.rcdefaults()\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "ners = ('Polyglot', 'GATE', 'Spacy', 'NLTK', 'Flair (12 class)', 'Allen NLP', 'Stanford Core NLP (4 class)', 'Deep Pavlov', 'Flair (4 class)', 'Stanford Core NLP (7 class)', 'Stanford Core NLP (3 class)')\n",
        "performance = [f1_poly, f1_gate, f1_spacy, f1_nltk, f1_flair12, f1_allen, f1_stanford2, f1_deeppavlov, f1_flair4, f1_stanford3, f1_stanford1]\n",
        "y_pos = np.arange(len(ners))\n",
        "\n",
        "ax.barh(y_pos, performance, align='center', color='orange', ecolor='black')\n",
        "ax.set_yticks(y_pos)\n",
        "ax.set_yticklabels(ners)\n",
        "ax.invert_yaxis()  \n",
        "ax.set_xlabel('F1 score')\n",
        "ax.set_title('How good is each NER at detecting entities?')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7J31lsMhONNW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Which is the best NER for correctly labelling entities?"
      ]
    },
    {
      "metadata": {
        "id": "HySRaiqa_5rj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "deeppavlov_people = {entity for entity,tag in Y_deeppavlov if tag == \"PERSON\"}\n",
        "flair4_people = {entity for entity,tag in Y_flair4 if tag == \"PER\"}\n",
        "flair12_people = {entity for entity,tag in Y_flair12 if tag == \"PERSON\"}\n",
        "gate_people = {entity.replace('(male)','').replace('(female)','').strip() for entity,tag in Y_gate if tag == \"Person\"}\n",
        "polyglot_people = {entity for entity,tag in Y_polyglot if tag == \"PER\"}\n",
        "spacy_people = {entity for entity,tag in Y_spacy if tag == \"PERSON\"}\n",
        "nltk_people = {entity for entity,tag in Y_nltk if tag == \"PERSON\"}\n",
        "stanford_people1 = {entity for entity,tag in Y_stanford1 if tag == \"PERSON\"}\n",
        "stanford_people2 = {entity for entity,tag in Y_stanford2 if tag == \"PERSON\"}\n",
        "stanford_people3 = {entity for entity,tag in Y_stanford3 if tag == \"PERSON\"}\n",
        "allen_people = {entity for entity,tag in Y_allen if tag == \"PERSON\"}\n",
        "\n",
        "deeppavlov_dates = {entity for entity,tag in Y_deeppavlov if tag == \"DATE\"}\n",
        "flair4_dates = {entity for entity,tag in Y_flair4 if tag == \"DATE\"}\n",
        "flair12_dates = {entity for entity,tag in Y_flair12 if tag == \"DATE\"}\n",
        "gate_dates = {entity for entity,tag in Y_gate if tag == \"Date\"}\n",
        "polyglot_dates = {entity for entity,tag in Y_polyglot if tag == \"DATE\"}\n",
        "spacy_dates = {entity for entity,tag in Y_spacy if tag == \"DATE\"}\n",
        "nltk_dates = {entity for entity,tag in Y_nltk if tag == \"DATE\"}\n",
        "stanford_dates1 = {entity for entity,tag in Y_stanford1 if tag == \"DATE\"}\n",
        "stanford_dates2 = {entity for entity,tag in Y_stanford2 if tag == \"DATE\"}\n",
        "stanford_dates3 = {entity for entity,tag in Y_stanford3 if tag == \"DATE\"}\n",
        "allen_dates = {entity for entity,tag in Y_allen if tag == \"DATE\"}\n",
        "\n",
        "deeppavlov_locations = {entity for entity,tag in Y_deeppavlov if tag == \"GPE\"}\n",
        "flair4_locations = {entity for entity,tag in Y_flair4 if tag == \"LOC\"}\n",
        "flair12_locations = {entity for entity,tag in Y_flair12 if tag == \"GPE\"}\n",
        "gate_locations = {entity for entity,tag in Y_gate if tag == \"Location\"}\n",
        "polyglot_locations = {entity for entity,tag in Y_polyglot if tag == \"LOC\"}\n",
        "spacy_locations = {entity for entity,tag in Y_spacy if tag == \"GPE\"}\n",
        "nltk_locations = {entity for entity,tag in Y_nltk if tag == \"GPE\"}\n",
        "stanford_locations1 = {entity for entity,tag in Y_stanford1 if tag == \"LOCATION\"}\n",
        "stanford_locations2 = {entity for entity,tag in Y_stanford2 if tag == \"LOCATION\"}\n",
        "stanford_locations3 = {entity for entity,tag in Y_stanford3 if tag == \"LOCATION\"}\n",
        "allen_locations = {entity for entity,tag in Y_allen if tag == \"GPE\"}\n",
        "\n",
        "f1_deeppavlov_ppl = f1_score(test[\"entities\"][\"people\"], deeppavlov_people)\n",
        "f1_deeppavlov_dts = f1_score(test[\"entities\"][\"dates\"], deeppavlov_dates)\n",
        "f1_deeppavlov_loc = f1_score(test[\"entities\"][\"locations\"], deeppavlov_locations)\n",
        "print(f\"Deep Pavlov\\n\\tPeople =\\t{f1_deeppavlov_ppl*100}%\\t{deeppavlov_people}\\n\\tLocations =\\t{f1_deeppavlov_loc*100}%\\t{deeppavlov_locations}\\n\\tDates =\\t\\t{f1_deeppavlov_dts*100}%\\t{deeppavlov_dates}\\n\\n\")\n",
        "f1_flair4_ppl = f1_score(test[\"entities\"][\"people\"], flair4_people)\n",
        "f1_flair4_dts = f1_score(test[\"entities\"][\"dates\"], flair4_dates)\n",
        "f1_flair4_loc = f1_score(test[\"entities\"][\"locations\"], flair4_locations)\n",
        "print(f\"Flair (4 class) \\n\\tPeople =\\t{f1_flair4_ppl*100}%\\t{flair4_people}\\n\\tLocations =\\t{f1_flair4_loc*100}%\\t{flair4_locations}\\n\\tDates =\\t\\t{f1_flair4_dts*100}%\\t{flair4_dates}\\n\\n\")\n",
        "f1_flair12_ppl = f1_score(test[\"entities\"][\"people\"], flair12_people)\n",
        "f1_flair12_dts = f1_score(test[\"entities\"][\"dates\"], flair12_dates)\n",
        "f1_flair12_loc = f1_score(test[\"entities\"][\"locations\"], flair12_locations)\n",
        "print(f\"Flair (12 class) \\n\\tPeople =\\t{f1_flair12_ppl*100}%\\t{flair12_people}\\n\\tLocations =\\t{f1_flair12_loc*100}%\\t{flair12_locations}\\n\\tDates =\\t\\t{f1_flair12_dts*100}%\\t{flair12_dates}\\n\\n\")\n",
        "f1_gate_ppl = f1_score(test[\"entities\"][\"people\"], gate_people)\n",
        "f1_gate_dts = f1_score(test[\"entities\"][\"dates\"], gate_dates)\n",
        "f1_gate_loc = f1_score(test[\"entities\"][\"locations\"], gate_locations)\n",
        "print(f\"GATE \\n\\tPeople =\\t{f1_gate_ppl*100}%\\t{gate_people}\\n\\tLocations =\\t{f1_gate_loc*100}%\\t{gate_locations}\\n\\tDates =\\t\\t{f1_gate_dts*100}%\\t{gate_dates}\\n\\n\")\n",
        "f1_poly_ppl = f1_score(test[\"entities\"][\"people\"], polyglot_people)\n",
        "f1_poly_dts = f1_score(test[\"entities\"][\"dates\"], polyglot_dates)\n",
        "f1_poly_loc = f1_score(test[\"entities\"][\"locations\"], polyglot_locations)\n",
        "print(f\"Polyglot \\n\\tPeople =\\t{f1_poly_ppl*100}%\\t{polyglot_people}\\n\\tLocations =\\t{f1_poly_loc*100}%\\t{polyglot_locations}\\n\\tDates =\\t\\t{f1_poly_dts*100}%\\t{polyglot_dates}\\n\\n\")\n",
        "f1_spacy_ppl = f1_score(test[\"entities\"][\"people\"], spacy_people)\n",
        "f1_spacy_dts = f1_score(test[\"entities\"][\"dates\"], spacy_dates)\n",
        "f1_spacy_loc = f1_score(test[\"entities\"][\"locations\"], spacy_locations)\n",
        "print(f\"Spacy \\n\\tPeople =\\t{f1_spacy_ppl*100}%\\t{spacy_people}\\n\\tLocations =\\t{f1_spacy_loc*100}%\\t{spacy_locations}\\n\\tDates =\\t\\t{f1_spacy_dts*100}%\\t{spacy_dates}\\n\\n\")\n",
        "f1_nltk_ppl = f1_score(test[\"entities\"][\"people\"], nltk_people)\n",
        "f1_nltk_dts = f1_score(test[\"entities\"][\"dates\"], nltk_dates)\n",
        "f1_nltk_loc = f1_score(test[\"entities\"][\"locations\"], nltk_locations)\n",
        "print(f\"NLTK \\n\\tPeople =\\t{f1_nltk_ppl*100}%\\t{nltk_people}\\n\\tLocations =\\t{f1_nltk_loc*100}%\\t{nltk_locations}\\n\\tDates =\\t\\t{f1_nltk_dts*100}%\\t{nltk_dates}\\n\\n\")\n",
        "f1_stanford_ppl1 = f1_score(test[\"entities\"][\"people\"], stanford_people1)\n",
        "f1_stanford_dts1 = f1_score(test[\"entities\"][\"dates\"], stanford_dates1)\n",
        "f1_stanford_loc1 = f1_score(test[\"entities\"][\"locations\"], stanford_locations1)\n",
        "print(f\"Stanford Core NLP (3 class)\\n\\tPeople =\\t{f1_stanford_ppl1*100}%\\t{stanford_people1}\\n\\tLocations =\\t{f1_stanford_loc1*100}%\\t{stanford_locations1}\\n\\tDates =\\t\\t{f1_stanford_dts1*100}%\\t{stanford_dates1}\\n\\n\")\n",
        "f1_stanford_ppl2 = f1_score(test[\"entities\"][\"people\"], stanford_people2)\n",
        "f1_stanford_dts2 = f1_score(test[\"entities\"][\"dates\"], stanford_dates2)\n",
        "f1_stanford_loc2 = f1_score(test[\"entities\"][\"locations\"], stanford_locations2)\n",
        "print(f\"Stanford Core NLP (4 class)\\n\\tPeople =\\t{f1_stanford_ppl2*100}%\\t{stanford_people2}\\n\\tLocations =\\t{f1_stanford_loc2*100}%\\t{stanford_locations2}\\n\\tDates =\\t\\t{f1_stanford_dts2*100}%\\t{stanford_dates2}\\n\\n\")\n",
        "f1_stanford_ppl3 = f1_score(test[\"entities\"][\"people\"], stanford_people3)\n",
        "f1_stanford_dts3 = f1_score(test[\"entities\"][\"dates\"], stanford_dates3)\n",
        "f1_stanford_loc3 = f1_score(test[\"entities\"][\"locations\"], stanford_locations3)\n",
        "print(f\"Stanford Core NLP (7 class)\\n\\tPeople =\\t{f1_stanford_ppl3*100}%\\t{stanford_people3}\\n\\tLocations =\\t{f1_stanford_loc3*100}%\\t{stanford_locations3}\\n\\tDates =\\t\\t{f1_stanford_dts3*100}%\\t{stanford_dates3}\\n\\n\")\n",
        "f1_allen_ppl = f1_score(test[\"entities\"][\"people\"], allen_people)\n",
        "f1_allen_dts = f1_score(test[\"entities\"][\"dates\"], allen_dates)\n",
        "f1_allen_loc = f1_score(test[\"entities\"][\"locations\"], allen_locations)\n",
        "print(f\"Allen NLP \\n\\tPeople =\\t{f1_allen_ppl*100}%\\t{allen_people}\\n\\tLocations =\\t{f1_allen_loc*100}%\\t{allen_locations}\\n\\tDates =\\t\\t{f1_allen_dts*100}%\\t{allen_dates}\\n\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_FYCrxS0jF5k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"NER\":[\"Deep Pavlov\",\"Flair (4 class)\", \"Flair (12 class)\", \"Polyglot\",\"GATE\",\"Spacy\",\"NLTK\",\"Allen\",\"Stanford (7 class)\",\"Stanford (3 class)\",\"Stanford (4 class)\"] *3,\n",
        "    \"Entity type\":[\"People\"] *11 + [\"Locations\"] *11 + [\"Dates\"]*11,\n",
        "    \"F1 score\":[\n",
        "        f1_deeppavlov_ppl, f1_flair4_ppl, f1_flair12_ppl, f1_poly_ppl, f1_gate_ppl, f1_spacy_ppl, f1_nltk_ppl, f1_allen_ppl,f1_stanford_ppl3,f1_stanford_ppl1,f1_stanford_ppl2, \n",
        "        f1_deeppavlov_loc, f1_flair4_loc, f1_flair12_loc, f1_poly_loc, f1_gate_loc, f1_spacy_loc, f1_nltk_loc, f1_allen_loc, f1_stanford_loc3,f1_stanford_loc1,f1_stanford_loc2, \n",
        "        f1_deeppavlov_dts, f1_flair4_dts, f1_flair12_dts, f1_poly_dts, f1_gate_dts, f1_spacy_dts, f1_nltk_dts,  f1_allen_dts, f1_stanford_dts3,f1_stanford_dts1,f1_stanford_dts2\n",
        "    ],\n",
        "})\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "sns.factorplot(x='F1 score',y='NER', hue='Entity type', data=df, kind='bar')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PeKNr09ZHz3C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Train your own NER"
      ]
    },
    {
      "metadata": {
        "id": "pb-2UEsmHcGq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load model from NLP Architect (Intel)"
      ]
    },
    {
      "metadata": {
        "id": "EMyRBZibcYvn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install nlp-architect\n",
        "!git clone https://github.com/NervanaSystems/nlp-architect.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eW8y1zRkHpUx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download some Labelled Training Data"
      ]
    },
    {
      "metadata": {
        "id": "ZmrSn-UgHoIN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget \"http://files.deeppavlov.ai/deeppavlov_data/conll2003_v2.tar.gz\"\n",
        "!tar xvzf conll2003_v2.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zHOizfPzHsEu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train model"
      ]
    },
    {
      "metadata": {
        "id": "stc3fXSKfh1g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 nlp-architect/examples/ner/train.py -e 3 --train_file train.txt --test_file test.txt --tag_num 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ye1aOCIVH00N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Use trained model"
      ]
    },
    {
      "metadata": {
        "id": "HwuWbl93f7Bs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 nlp-architect/examples/ner/interactive.py --model_path model.h5 --model_info_path model_info.dat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Muu4I6nAH5iB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Format results"
      ]
    },
    {
      "metadata": {
        "id": "j14NWmvFn4O7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nlp_architect.models.ner_crf import NERCRF\n",
        "model = NERCRF()\n",
        "model.load(\"model.h5\")\n",
        "\n",
        "import pickle\n",
        "with open(\"model_info.dat\", 'rb') as fp:\n",
        "  model_info = pickle.load(fp)\n",
        "word_vocab = model_info['word_vocab']\n",
        "y_vocab = {v: k for k, v in model_info['y_vocab'].items()}\n",
        "char_vocab = model_info['char_vocab']\n",
        "\n",
        "from nlp_architect.utils.text import SpacyInstance\n",
        "nlp = SpacyInstance(disable=['tagger', 'ner', 'parser', 'vectors', 'textcat'])\n",
        "\n",
        "import numpy as np\n",
        "from nlp_architect.utils.generic import pad_sentences\n",
        "def vectorize(doc, w_vocab, c_vocab):\n",
        "    words = np.asarray([w_vocab[w.lower()] if w.lower() in w_vocab else 1 for w in doc]).reshape(1, -1)\n",
        "    sentence_chars = []\n",
        "    for w in doc:\n",
        "        word_chars = []\n",
        "        for c in w:\n",
        "            if c in c_vocab:\n",
        "                _cid = c_vocab[c]\n",
        "            else:\n",
        "                _cid = 1\n",
        "            word_chars.append(_cid)\n",
        "        sentence_chars.append(word_chars)\n",
        "    sentence_chars = np.expand_dims(pad_sentences(sentence_chars, model.word_length), axis=0)\n",
        "    return words, sentence_chars\n",
        "\n",
        "def your_ner(text):\n",
        "  text_arr = nlp.tokenize(text)\n",
        "  doc_vec = vectorize(text_arr, word_vocab, char_vocab)\n",
        "  seq_len = np.array([len(text_arr)]).reshape(-1, 1)\n",
        "  inputs = list(doc_vec)\n",
        "  if model.crf_mode == 'pad':\n",
        "    inputs = list(doc_vec) + [seq_len]\n",
        "  doc_ner = model.predict(inputs, batch_size=1).argmax(2).flatten()\n",
        "  ners = [y_vocab.get(n, None) for n in doc_ner]\n",
        "  return convert_entities([(t,n.strip()) for t,n in zip(text_arr, ners)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "78Nq1e7TpaCU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "your_ner(example_document)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5N0GnUuJyZvZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "your_ner(test[\"text\"])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}