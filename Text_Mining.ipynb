{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Mining.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammedterry/NLP_Lab/blob/master/Text_Mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "E5jzBMCalDsT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "def display(ranked_wordlist, n_samples = 15):\n",
        "  sns.barplot(x=\"frequency\", y=\"words\", data=pd.DataFrame({\"frequency\":[x for x,_ in ranked_wordlist[:n_samples]], \"words\":[x for _,x in ranked_wordlist[:n_samples]]}))\n",
        "  \n",
        "def compare(ranked_wordlist_a, ranked_wordlist_b, method_a = \"a\", method_b = \"b\", n_samples = 15):\n",
        "  d_i = pd.DataFrame({\n",
        "    \"method\":[method_a]*n_samples + [method_b]*n_samples,\n",
        "    \"frequency\":[x for x,_ in ranked_wordlist_a[:n_samples]] + [x for x,_ in ranked_wordlist_b[:n_samples]],\n",
        "    \"words\":[x for _,x in ranked_wordlist_a[:n_samples]] + [x for _,x in ranked_wordlist_b[:n_samples]], \n",
        "  })\n",
        "  sns.factorplot(\"frequency\", \"words\", col=\"method\", data=d_i, kind=\"bar\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VSijAIUcmFeV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Get some raw text"
      ]
    },
    {
      "metadata": {
        "id": "zNTuY819mE1m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "\n",
        "def scrape(url):\n",
        "  soup = bs(requests.get(url, headers = {'user-agent': \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\",'referrer': 'https://google.com'}).text, \"html.parser\")\n",
        "  return '\\n'.join([dom.text for dom in soup.find_all(['title','h1','h2','h3','h4','h5','h6','p'])])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BGTH4tdpmxpd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "example_document = scrape(\"http://www.literatureproject.com/alice/alice_7.htm\")\n",
        "\n",
        "print(example_document[:300])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hYcsM4UtkTxL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Frequency of Terms"
      ]
    },
    {
      "metadata": {
        "id": "-EW8yEm_kX6j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def rank_words(document):\n",
        "  counter = defaultdict(int)\n",
        "  for word in document.split():\n",
        "      counter[word] += 1\n",
        "  return sorted(zip(counter.values(), counter.keys()), reverse = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3FsSk0Dwkiaw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ranked_raw = rank_words(example_document)\n",
        "display(ranked_raw)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SQu7EDsspUIa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Strip Punctuation "
      ]
    },
    {
      "metadata": {
        "id": "H2gtUIGipYA_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def clean(text):\n",
        "  return ' '.join(''.join(letter if ord('a') <= ord(letter.lower()) <= ord('z') or letter.isdigit() else ' ' for letter in text).split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NQv9ZD_8pqow",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "clean_document = clean(example_document)\n",
        "clean_document[:100]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jSa_IE5UqYoo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lower Case"
      ]
    },
    {
      "metadata": {
        "id": "VGt2KbRcqZ5P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "clean_document = clean_document.lower()\n",
        "clean_document[:100]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ECTKitnotcec",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ranked_clean = rank_words(clean_document)\n",
        "display(ranked_clean)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w9ekHDBBpuHl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "compare(ranked_raw,ranked_clean, \"raw\", \"lower case\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VogWiYNRtzh4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Split Strings"
      ]
    },
    {
      "metadata": {
        "id": "ptjMWEbq2vrs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install wordninja"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oufIgFRq2tM7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from wordninja import split\n",
        "\n",
        "x = 'itwasadarkandstormynighttherainfellintorrentsexceptatoccasionalintervalswhenitwascheckedbyaviolentgustofwindwhichsweptupthestreetsforitisinlondonthatoursceneliesrattlingalongthehousetopsandfiercelyagitatingthescantyflameofthelampsthatstruggledagainstthedarkness'\n",
        "' '.join(split(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pDnURB6h28D2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "split_document = ' '.join(split(clean_document))\n",
        "ranked_split = rank_words(split_document)\n",
        "display(ranked_split)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1jiHNpXL3ND8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "compare(ranked_clean, ranked_split, \"clean\", \"split\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SU6K0s6q3kAa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Stop Words"
      ]
    },
    {
      "metadata": {
        "id": "nIFrMoIA31Ez",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hk8l-vaS36tv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopWords = stopwords.words('english')\n",
        "stopWords[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-e3Unw_W4ETJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nostopwords_document = ' '.join([word for word in split_document.split() if word not in stopWords])\n",
        "ranked_nostopwords = rank_words(nostopwords_document)\n",
        "display(ranked_nostopwords)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fuZ-4FqU409-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "compare(ranked_split,ranked_nostopwords, \"split\", \"no stop words\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TQIP7TAJ31mo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# N-Grams"
      ]
    },
    {
      "metadata": {
        "id": "cF16v7Cg830o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "list(nltk.bigrams(\"this is a test\".split()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XEHLxP_o86eb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "list(nltk.trigrams(\"this is a test\".split()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w6UBoD8O7b_h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import nltk\n",
        "\n",
        "def rank_ngrams(document, unigrams = True, bigrams = True, trigrams = True):\n",
        "  counter = defaultdict(int)\n",
        "  if unigrams:\n",
        "    for word in document.split():\n",
        "        counter[word] += 1\n",
        "  if bigrams:\n",
        "    for bigram in nltk.bigrams(document.split()):\n",
        "      counter[' '.join(bigram)] += 1\n",
        "  if trigrams:\n",
        "    for trigram in nltk.trigrams(document.split()):\n",
        "      counter[' '.join(trigram)] += 1\n",
        "  return sorted(zip(counter.values(), counter.keys()), reverse = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rbeu-PjQ7LfO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ranked_ngrams = rank_ngrams(nostopwords_document)\n",
        "display(ranked_ngrams)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rrZzqLyl9M-u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "compare(ranked_nostopwords, ranked_ngrams, method_b = \"n grams\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ozixvPV16BMg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Stemming\n",
        "chopping off suffixes"
      ]
    },
    {
      "metadata": {
        "id": "n97KKi2y9g7p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('rslp')\n",
        "\n",
        "from nltk.stem.api import StemmerI\n",
        "from nltk.stem.regexp import RegexpStemmer\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.rslp import RSLPStemmer\n",
        "\n",
        "stemmer_sb = SnowballStemmer(\"english\")\n",
        "stemmer_p = PorterStemmer()\n",
        "stemmer_l = LancasterStemmer()\n",
        "stemmer_regex = RegexpStemmer('ing$|s$|e$|able$|ed$|en$',min=4)\n",
        "stemmer_isri = ISRIStemmer()\n",
        "stemmer_rslp = RSLPStemmer()\n",
        "\n",
        "def get_name(model):\n",
        "  name = clean(str(model).split()[0])\n",
        "  name = ''.join([c + ' ' if d.isupper() and e.islower() else c for c,d,e in zip(name,name[1:] + '..',name[2:] + '..')])\n",
        "  return ' '.join(name.split())\n",
        "  \n",
        "def test_stemmers(Xs, Ys, stemmers = [stemmer_sb, stemmer_p, stemmer_l, stemmer_isri, stemmer_rslp, stemmer_regex]):\n",
        "  results = {\"expected\":Ys}\n",
        "  for stemmer in stemmers:\n",
        "    results[get_name(stemmer)] = [stemmer.stem(x.lower()) for x in Xs]\n",
        "  for model,words in results.items():\n",
        "    results[model].append(sum([w1.lower() == w2.lower() for w1,w2 in zip(results['expected'],words)]))  \n",
        "\n",
        "  import pandas as pd\n",
        "  df = pd.DataFrame(results)\n",
        "  df.set_index(\"expected\", inplace = True) \n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K1blZP2vdZLU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "example_sentence = \"Sitting while eating leaves one wondering and then googling\"\n",
        "expected_sentence = \"Sit while eat leave one wonder and then google\"\n",
        "test_stemmers(example_sentence.split(), expected_sentence.split() )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uA7mDpfYmr7I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lemmatising\n",
        "getting to the root of the word"
      ]
    },
    {
      "metadata": {
        "id": "ThRmSgsFd4lN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem.wordnet import WordNetLemmatizer \n",
        "from spacy.lemmatizer import Lemmatizer \n",
        "\n",
        "wn_lemmatiser = WordNetLemmatizer()\n",
        "sp_lemmatiser = Lemmatizer()\n",
        "\n",
        "def test_lemmatisers(Xs, Ys):\n",
        "  lemmatiser_results = {\n",
        "      \"expected\":Ys,\n",
        "      \"NLTK\":[wn_lemmatiser.lemmatize(x.lower(),'v') for x in Xs],\n",
        "      \"Spacy\":[sp_lemmatiser(x.lower(), 'VERB')[0] for x in Xs],    \n",
        "  }\n",
        "\n",
        "  for lemmatiser,words in lemmatiser_results.items():\n",
        "    lemmatiser_results[lemmatiser].append(sum([w1.lower() == w2.lower() for w1,w2 in zip(lemmatiser_results['expected'],words)]))  \n",
        "\n",
        "  import pandas as pd\n",
        "  lemma_df = pd.DataFrame(lemmatiser_results)\n",
        "  lemma_df.set_index(\"expected\", inplace = True) \n",
        "  return lemma_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e1pv-dQinhy0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "example_sentence = \"Sitting while eating leaves one wondering and then googling\"\n",
        "expected_sentence = \"Sit while eat leave one wonder and then google\"\n",
        "test_lemmatisers(example_sentence.split(), expected_sentence.split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ToSiET8ktXjx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lemmatised_document = ' '.join([wn_lemmatiser.lemmatize(word,'v') for word in nostopwords_document.split()])\n",
        "ranked_lemma = rank_ngrams(lemmatised_document)\n",
        "display(ranked_lemma)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xE4GRdmUuE38",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "compare(ranked_ngrams, ranked_lemma,'pre-lemmatisation', 'post-lemmatisation')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3w2U1AKY6vWl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Tf-Idf"
      ]
    },
    {
      "metadata": {
        "id": "3Wqip1YtvQL4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "alice_wonderland_12chapters = [scrape(f\"http://www.literatureproject.com/alice/alice_{i}.htm\") for i in range(1,13)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BuoBpF9Ov1iq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "alice_wonderland_12chapters[6][:100]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tzXhwemp6uxI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def rank_tfidf(document, tfidf):\n",
        "  feature_names = tfidf.get_feature_names()\n",
        "  response = tfidf.transform([document])\n",
        "  tfs = {feature_names[col] : response[0, col]  for col in response.nonzero()[1]}\n",
        "  return sorted(zip(tfs.values(), tfs.keys()), reverse = True)\n",
        "\n",
        "tfidf = TfidfVectorizer(stop_words = 'english', ngram_range = (1,3))\n",
        "tfidf.fit_transform(alice_wonderland_12chapters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q8aHKZF6wUY9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ranked_tfidf = rank_tfidf(example_document, tfidf)\n",
        "display(ranked_tfidf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JJKmmnVfxSKB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "max_freq = max(ranked_ngrams)[0] * 3\n",
        "scaled_tfidf = [(freq*max_freq,word) for freq,word in ranked_tfidf]\n",
        "compare(ranked_ngrams, scaled_tfidf, \"pre-tfidf\", \"post-tfidf\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cwsmn5Q0tTNm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Fuzzy String Matching"
      ]
    },
    {
      "metadata": {
        "id": "ga8E7OIc1hh6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "useful for typos"
      ]
    },
    {
      "metadata": {
        "id": "qFi3u5prvSjX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Word Vectors"
      ]
    },
    {
      "metadata": {
        "id": "KpRDNCy81jLW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "useful for extracting meaning"
      ]
    }
  ]
}